#FONTE DE CONSULTA: DOCUMENTAÇAO SPARK  E stackoverflow
#https://spark.apache.org/docs/2.1.0/sql-programming-guide.html
#https://stackoverflow.com


#1.Número de hosts únicos(ARQUIVO JULHO)  = 81982 HOSTS
#APLICANDO O MEMSO CÓDIGO ABAIXO PARA O ARQUIVO AGOSTO = 75060 HOSTS

#NO CODIGO PYSPARK ABAIXO CRIEI UM DATAFRAME A PARTIR DO ARQUIVO PARA REALIZAR UMA QUERY 

from pyspark.sql.types import *
sc = spark.sparkContext
lines =sc.textFile("/Users/Paulo/Downloads/NASA_access_log_Jul95")

#VERIFICANDO A ESTRUTURA DA PRIMEIRA LINHA DO ARQUIVO
lines.first()

parts = lines.map(lambda l: l.split(" "))

#UTILIZANDO O NÚMERO DE CAMPOS PRESUMIDO OU CONSULTAR A PRIMEIRA LINHA DO ARQUIVO 
hosts = parts.map(lambda p: (p[0], p[1],p[2],p[3],p[4],p[5],p[6].strip()))

schemaString = "HOST DUMB DUMB1 DATA REQ CODIGO TOTAL"
fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)
schemaHosts = spark.createDataFrame(hosts,schema)
schemaHosts.createOrReplaceTempView("hosts")

#QUERY RETORNANDO QUANTIDADE DE HOSTS DISTINTOS
results =  spark.sql("SELECT COUNT(DISTINCT HOST) FROM hosts")
results.show()
+--------------------+                                                          
|count(DISTINCT HOST)|
+--------------------+
|               81982|
+--------------------+

2. O TOTAL DE ERROS 404 

julho = 10846
agosto = 10057

#SCALA julho
scala> val nasajulho = sc.textFile("/Users/Paulo/Downloads/NASA_access_log_Jul95")
scala> val errosjulho = nasajulho.filter(line => line.contains("404 "))
scala> errosjulho.count()
res3: Long = 10846 

#SCALA agosto
scala> val agosto= sc.textFile("/Users/Paulo/Downloads/NASA_access_log_Aug95")
scala> val errosagosto = agosto.filter(line=>line.contains("404 "))
scala> errosagosto.count()
res1: Long = 10057   



#CONSIDERAÇOES 

APÓS A PRIMEIRA IMPLEMENTAÇÃO DOS DATAFRAMES, VERIFIQUEI QUE PARA CONSIDRERAR O DELIMITADOR " "(ESPAÇO) 
E EXIBIR TODAS AS INFORMAÇÕES TABULARMENTE, SÃO NECESSÁRIOS 10 CAMPOS

#apos modificar a quantidade de campos no trecho ilustrado a seguir:
hosts = parts.map(lambda p: (p[0],p[1],p[2],p[3],p[4],p[5],p[6],p[7],p[8],p[9].strip()))
schemaString = "HOST DUMB DUMB1 DATA TIMEZONE ACAO ENDERECO PROTOCOL  CODIGO BYTES"

>>> result = spark.sql("SELECT * FROM hosts")
>>> result.show()
+--------------------+----+-----+--------------------+--------+----+--------------------+---------+------+-----+
|                HOST|DUMB|DUMB1|                DATA|TIMEZONE|ACAO|            ENDERECO| PROTOCOL|CODIGO|BYTES|
+--------------------+----+-----+--------------------+--------+----+--------------------+---------+------+-----+
|        199.72.81.55|   -|    -|[01/Jul/1995:00:0...|  -0400]|"GET|    /history/apollo/|HTTP/1.0"|   200| 6245|

CONSEGUI RETORNAR TODAS AS INFORMAÇÕES DESEJADAS PORÉM AS QUERIES COM FILTROS DEIXAM DE FUNCIONAR DEVIDO AO ERRO :
IndexError: list index out of range

Não localizei um workaround o que impossibilitou a solução das outras 3 questões.

